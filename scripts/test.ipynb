{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "772e1832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/01 17:22:01 INFO mlflow.tracking.fluent: Experiment with name 'housing_price_training' does not exist. Creating a new experiment.\n",
      "2025-11-01 17:22:02,059 INFO housing - Data path: d:\\Boothcamp_Mlops\\data\\housing.csv\n",
      "2025-11-01 17:22:02,060 INFO housing - Artifact dir: d:\\Boothcamp_Mlops\\artifacts\n",
      "2025-11-01 17:22:02,061 INFO housing - Loading dataset...\n",
      "2025-11-01 17:22:02,108 INFO housing - Loaded 5000 rows and 7 columns\n",
      "2025-11-01 17:22:02,111 INFO housing - Preparing features and target...\n",
      "2025-11-01 17:22:02,145 INFO housing - Splitting train/test...\n",
      "2025-11-01 17:22:02,159 INFO housing - Building pipeline...\n",
      "2025-11-01 17:22:02,161 INFO housing - Training model...\n",
      "2025-11-01 17:22:02,815 INFO housing - Logged hyperparameters: {'max_iter': 5000, 'tol': 0.001, 'learning_rate': 'optimal', 'random_state': 20, 'model_name': 'SGDRegressor'}\n",
      "2025-11-01 17:22:02,859 INFO housing - Evaluating model performance...\n",
      "2025-11-01 17:22:02,877 INFO housing - Model performance metrics:\n",
      "2025-11-01 17:22:02,898 INFO housing -  Â train_mse: 11642367609.1414\n",
      "2025-11-01 17:22:02,923 INFO housing -  Â train_mae: 86824.7124\n",
      "2025-11-01 17:22:02,945 INFO housing -  Â train_r2: 0.9069\n",
      "2025-11-01 17:22:02,962 INFO housing -  Â test_mse: 11370845501.5434\n",
      "2025-11-01 17:22:02,979 INFO housing -  Â test_mae: 85739.9224\n",
      "2025-11-01 17:22:02,998 INFO housing -  Â test_r2: 0.9076\n",
      "2025-11-01 17:22:03,027 INFO housing -  Â train_rmse: 107899.8036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 5819269802357.51, NNZs: 5, Bias: -284411575081.759277, T: 4000, Avg. loss: 133507662928172757676982272.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4101317096933.26, NNZs: 5, Bias: 2646851901885.573730, T: 8000, Avg. loss: 13719414432743692057968640.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2803091923143.66, NNZs: 5, Bias: -83373759522.158325, T: 12000, Avg. loss: 4560999936234764808224768.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2854502212035.48, NNZs: 5, Bias: -440619445678.418335, T: 16000, Avg. loss: 2136594426981578010787840.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1607096821261.89, NNZs: 5, Bias: -818641017402.999146, T: 20000, Avg. loss: 1140891627127447455006720.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 902361242653.86, NNZs: 5, Bias: 605603697280.083252, T: 24000, Avg. loss: 633722000965620960591872.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 987329948470.51, NNZs: 5, Bias: 129608060348.854065, T: 28000, Avg. loss: 359925709487582061002752.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 404601532473.27, NNZs: 5, Bias: -129069705306.616714, T: 32000, Avg. loss: 135590957276011964661760.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 356531.21, NNZs: 5, Bias: 1361337.139170, T: 36000, Avg. loss: 3950661337052770992128.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 883382.72, NNZs: 5, Bias: 1083596.300050, T: 40000, Avg. loss: 102929169381.843994\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 407246.87, NNZs: 5, Bias: 1156624.255929, T: 44000, Avg. loss: 32727006293.008183\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 352490.00, NNZs: 5, Bias: 1197058.147146, T: 48000, Avg. loss: 26430122664.264423\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 278372.10, NNZs: 5, Bias: 1182454.554668, T: 52000, Avg. loss: 16727631425.640383\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 335800.52, NNZs: 5, Bias: 1198364.077384, T: 56000, Avg. loss: 13896354937.208754\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 325587.46, NNZs: 5, Bias: 1216635.290527, T: 60000, Avg. loss: 12131260609.651094\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 363908.07, NNZs: 5, Bias: 1268426.605779, T: 64000, Avg. loss: 11638319949.881786\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 418251.05, NNZs: 5, Bias: 1252717.917181, T: 68000, Avg. loss: 10487228603.256565\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 432421.97, NNZs: 5, Bias: 1200813.469807, T: 72000, Avg. loss: 9920643972.831554\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 276012.89, NNZs: 5, Bias: 1201063.836568, T: 76000, Avg. loss: 9171055711.355490\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 405819.78, NNZs: 5, Bias: 1224721.289570, T: 80000, Avg. loss: 9099722391.921495\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 314966.48, NNZs: 5, Bias: 1257471.602670, T: 84000, Avg. loss: 8395256345.337255\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 360325.64, NNZs: 5, Bias: 1223819.472281, T: 88000, Avg. loss: 8315549722.298304\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 361399.91, NNZs: 5, Bias: 1225438.218689, T: 92000, Avg. loss: 7995711907.975813\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 342347.86, NNZs: 5, Bias: 1240610.658970, T: 96000, Avg. loss: 7991699659.628661\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 364991.31, NNZs: 5, Bias: 1208042.670420, T: 100000, Avg. loss: 7722810618.358390\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 334948.76, NNZs: 5, Bias: 1260373.377594, T: 104000, Avg. loss: 7691589863.088940\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 306664.89, NNZs: 5, Bias: 1253227.461724, T: 108000, Avg. loss: 7275007891.112467\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 329880.38, NNZs: 5, Bias: 1216803.482388, T: 112000, Avg. loss: 7121686977.091701\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 349848.95, NNZs: 5, Bias: 1201701.585087, T: 116000, Avg. loss: 7181916016.162020\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 342357.55, NNZs: 5, Bias: 1201678.736080, T: 120000, Avg. loss: 7038465004.875857\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 334531.73, NNZs: 5, Bias: 1221512.636254, T: 124000, Avg. loss: 6838846337.371378\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 297306.09, NNZs: 5, Bias: 1205621.751528, T: 128000, Avg. loss: 6818629090.613236\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 343093.66, NNZs: 5, Bias: 1234079.726794, T: 132000, Avg. loss: 6760755710.730206\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 335255.25, NNZs: 5, Bias: 1254427.868850, T: 136000, Avg. loss: 6791456611.741123\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 346997.82, NNZs: 5, Bias: 1278036.942707, T: 140000, Avg. loss: 6621835732.609868\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 351503.81, NNZs: 5, Bias: 1207784.345906, T: 144000, Avg. loss: 6693718205.053440\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 374879.64, NNZs: 5, Bias: 1217377.239868, T: 148000, Avg. loss: 6691970636.895180\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 361204.49, NNZs: 5, Bias: 1230296.594626, T: 152000, Avg. loss: 6565238591.282021\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 346956.58, NNZs: 5, Bias: 1212209.348868, T: 156000, Avg. loss: 6420123438.259274\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 389109.12, NNZs: 5, Bias: 1249196.467928, T: 160000, Avg. loss: 6447873836.515310\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 352935.89, NNZs: 5, Bias: 1236801.175597, T: 164000, Avg. loss: 6339088065.985227\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 356195.02, NNZs: 5, Bias: 1226026.733231, T: 168000, Avg. loss: 6275653728.052668\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 342898.44, NNZs: 5, Bias: 1202897.898945, T: 172000, Avg. loss: 6324162585.585317\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 349637.25, NNZs: 5, Bias: 1208438.324550, T: 176000, Avg. loss: 6185245104.392927\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 347681.97, NNZs: 5, Bias: 1212590.738887, T: 180000, Avg. loss: 6240977657.051972\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 333484.70, NNZs: 5, Bias: 1213883.815523, T: 184000, Avg. loss: 6229492290.250236\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 364840.53, NNZs: 5, Bias: 1228564.238440, T: 188000, Avg. loss: 6192657441.406054\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 343743.99, NNZs: 5, Bias: 1224062.918242, T: 192000, Avg. loss: 6240733761.034624\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 396322.76, NNZs: 5, Bias: 1228813.932390, T: 196000, Avg. loss: 6115521277.570001\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 339809.83, NNZs: 5, Bias: 1214958.207736, T: 200000, Avg. loss: 6082282348.306920\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 336511.32, NNZs: 5, Bias: 1210995.697890, T: 204000, Avg. loss: 6060396276.319392\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 344288.37, NNZs: 5, Bias: 1248704.848889, T: 208000, Avg. loss: 6075227244.093527\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 336705.08, NNZs: 5, Bias: 1250925.498663, T: 212000, Avg. loss: 6006306504.424906\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 360960.86, NNZs: 5, Bias: 1207360.352252, T: 216000, Avg. loss: 5969351101.803924\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 342149.54, NNZs: 5, Bias: 1216154.979707, T: 220000, Avg. loss: 5944709344.098267\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 337046.41, NNZs: 5, Bias: 1205057.424716, T: 224000, Avg. loss: 5975408647.559227\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 344964.85, NNZs: 5, Bias: 1225154.190660, T: 228000, Avg. loss: 5883500747.818618\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 351902.88, NNZs: 5, Bias: 1208651.739391, T: 232000, Avg. loss: 5887062380.447742\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 335321.64, NNZs: 5, Bias: 1233518.032555, T: 236000, Avg. loss: 5917597435.899063\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 354563.37, NNZs: 5, Bias: 1238194.487923, T: 240000, Avg. loss: 5869537927.220463\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 352344.94, NNZs: 5, Bias: 1233668.394058, T: 244000, Avg. loss: 5868136113.600874\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 345332.08, NNZs: 5, Bias: 1201334.205224, T: 248000, Avg. loss: 5882016216.800331\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 348444.86, NNZs: 5, Bias: 1232199.384598, T: 252000, Avg. loss: 5885880471.632286\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 356849.95, NNZs: 5, Bias: 1228642.812208, T: 256000, Avg. loss: 5898265566.685888\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 330832.78, NNZs: 5, Bias: 1245083.789532, T: 260000, Avg. loss: 5851602684.426626\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 332190.56, NNZs: 5, Bias: 1254341.671374, T: 264000, Avg. loss: 5837576453.738747\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 370929.29, NNZs: 5, Bias: 1241032.431452, T: 268000, Avg. loss: 5761700935.466565\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 311416.31, NNZs: 5, Bias: 1227044.851809, T: 272000, Avg. loss: 5759710770.705579\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 335034.54, NNZs: 5, Bias: 1217462.973719, T: 276000, Avg. loss: 5808535602.064315\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 332458.37, NNZs: 5, Bias: 1239257.698947, T: 280000, Avg. loss: 5812844406.190189\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 335750.06, NNZs: 5, Bias: 1227567.897876, T: 284000, Avg. loss: 5768284324.732411\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 348352.64, NNZs: 5, Bias: 1256902.680041, T: 288000, Avg. loss: 5751665741.044711\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 351602.92, NNZs: 5, Bias: 1220672.231465, T: 292000, Avg. loss: 5732518216.763572\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 364436.38, NNZs: 5, Bias: 1224153.907822, T: 296000, Avg. loss: 5688069312.671632\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 340877.30, NNZs: 5, Bias: 1238039.334413, T: 300000, Avg. loss: 5768074293.574999\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 344220.60, NNZs: 5, Bias: 1226470.459121, T: 304000, Avg. loss: 5685831631.595715\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 348369.03, NNZs: 5, Bias: 1201976.680497, T: 308000, Avg. loss: 5724935099.982391\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 339770.61, NNZs: 5, Bias: 1255576.145957, T: 312000, Avg. loss: 5696136152.471195\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 360537.70, NNZs: 5, Bias: 1231919.172110, T: 316000, Avg. loss: 5682462333.680096\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 375298.53, NNZs: 5, Bias: 1226926.005875, T: 320000, Avg. loss: 5685448102.030319\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 323334.85, NNZs: 5, Bias: 1245092.722789, T: 324000, Avg. loss: 5653047073.479649\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 358241.92, NNZs: 5, Bias: 1229317.331515, T: 328000, Avg. loss: 5611386975.135476\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 331191.08, NNZs: 5, Bias: 1214562.770977, T: 332000, Avg. loss: 5615101707.840355\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 346628.07, NNZs: 5, Bias: 1227445.339119, T: 336000, Avg. loss: 5628167469.445969\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 334948.05, NNZs: 5, Bias: 1224061.344907, T: 340000, Avg. loss: 5679807963.567779\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 347268.06, NNZs: 5, Bias: 1251107.251702, T: 344000, Avg. loss: 5688050211.110405\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 334957.64, NNZs: 5, Bias: 1245371.000871, T: 348000, Avg. loss: 5627093456.005305\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 87 epochs took 0.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 17:22:03,050 INFO housing -  Â test_rmse: 106634.1667\n",
      "2025-11-01 17:22:03,212 INFO housing - Model saved locally and logged as artifact: d:\\Boothcamp_Mlops\\artifacts\\housing_linear.joblib\n",
      "2025/11/01 17:22:03 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/01 17:22:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025-11-01 17:22:12,076 INFO housing - Model logged using mlflow.sklearn.log_model\n",
      "2025-11-01 17:22:12,419 INFO housing - \n",
      "âœ… HoÃ n thÃ nh Run ID: d9949e29a3eb42f1aafa76c5db9dd105 trong Experiment: housing_price_training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run SGD_optimal_iter5000 at: http://127.0.0.1:5000/#/experiments/856326979871672178/runs/d9949e29a3eb42f1aafa76c5db9dd105\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/856326979871672178\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cáº¥u hÃ¬nh MLflow Tracking Server\n",
    "# Äáº£m báº£o server Ä‘ang cháº¡y táº¡i Ä‘á»‹a chá»‰ nÃ y (mlflow ui --port 5001)\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\") \n",
    "\n",
    "# Cáº¥u hÃ¬nh logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"housing\")\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Táº£i dá»¯ liá»‡u, huáº¥n luyá»‡n mÃ´ hÃ¬nh SGDRegressor, Ä‘Ã¡nh giÃ¡ vÃ  log káº¿t quáº£\n",
    "    vÃ  mÃ´ hÃ¬nh vÃ o MLflow Tracking Server.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Cáº¥u hÃ¬nh ÄÆ°á»ng dáº«n\n",
    "    mlflow.set_experiment(\"housing_price_training\")\n",
    "    \n",
    "    # PROJECT_ROOT sáº½ lÃ  thÆ° má»¥c D:\\Boothcamp_Mlops náº¿u báº¡n cháº¡y tá»« Ä‘Ã³\n",
    "    PROJECT_ROOT = Path(os.getcwd())\n",
    "    DATA_PATH = PROJECT_ROOT / \"data\" / \"housing.csv\"\n",
    "    \n",
    "    # Sá»¬A: Thay Ä‘á»•i ARTIFACT_DIR Ä‘á»ƒ lÆ°u trá»¯ á»Ÿ thÆ° má»¥c 'artifacts' cáº¥p cao nháº¥t\n",
    "    ARTIFACT_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "    ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MODEL_PATH = ARTIFACT_DIR / \"housing_linear.joblib\"\n",
    "\n",
    "    logger.info(f\"Data path: {DATA_PATH}\")\n",
    "    logger.info(f\"Artifact dir: {ARTIFACT_DIR}\")\n",
    "\n",
    "    # 2. Táº£i vÃ  Chuáº©n bá»‹ Dá»¯ liá»‡u\n",
    "    if not DATA_PATH.exists():\n",
    "        logger.error(f\"Dá»¯ liá»‡u khÃ´ng tÃ¬m tháº¥y táº¡i: {DATA_PATH}\")\n",
    "        return # Dá»«ng náº¿u khÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u\n",
    "\n",
    "    logger.info(\"Loading dataset...\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    logger.info(f\"Loaded {len(df)} rows and {len(df.columns)} columns\")\n",
    "\n",
    "    logger.info(\"Preparing features and target...\")\n",
    "    TARGET = \"Price\"\n",
    "    NUM_FEATURES = [\n",
    "        \"Avg. Area Income\",\n",
    "        \"Avg. Area House Age\",\n",
    "        \"Avg. Area Number of Rooms\",\n",
    "        \"Avg. Area Number of Bedrooms\",\n",
    "        \"Area Population\",\n",
    "    ]\n",
    "\n",
    "    X = df[NUM_FEATURES]\n",
    "    y = df[TARGET]\n",
    "\n",
    "    logger.info(\"Splitting train/test...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # 3. XÃ¢y dá»±ng Pipeline\n",
    "    logger.info(\"Building pipeline...\")\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), NUM_FEATURES),\n",
    "            # ThÃªm cÃ¡c bÆ°á»›c xá»­ lÃ½ dá»¯ liá»‡u khÃ¡c (nhÆ° OneHotEncoder cho CAT_FEATURES) táº¡i Ä‘Ã¢y náº¿u cáº§n\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "    \n",
    "    # Tham sá»‘ mÃ´ hÃ¬nh\n",
    "    max_iter = 5000\n",
    "    tol = 1e-3\n",
    "    learning_rate = \"optimal\"\n",
    "    random_state = 20\n",
    "    \n",
    "    # NhÃ³m cÃ¡c tham sá»‘ Ä‘á»ƒ log dá»… dÃ ng hÆ¡n\n",
    "    hyperparams = {\n",
    "        \"max_iter\": max_iter,\n",
    "        \"tol\": tol,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"random_state\": random_state,\n",
    "        \"model_name\": \"SGDRegressor\"\n",
    "    }\n",
    "\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\n",
    "                \"regressor\",\n",
    "                SGDRegressor(\n",
    "                    max_iter=max_iter,\n",
    "                    tol=tol,\n",
    "                    learning_rate=learning_rate,\n",
    "                    random_state=random_state,\n",
    "                    verbose=1, # Giá»¯ láº¡i Ä‘á»ƒ xem quÃ¡ trÃ¬nh fit\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 4. Huáº¥n luyá»‡n vÃ  Log vá»›i MLflow\n",
    "    logger.info(\"Training model...\")\n",
    "    # TÃªn run nÃªn bao gá»“m cÃ¡c tham sá»‘ chÃ­nh Ä‘á»ƒ dá»… phÃ¢n biá»‡t\n",
    "    run_name = f\"SGD_{learning_rate}_iter{max_iter}\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        \n",
    "        # Sá»¬A: Log Hyperparameters báº±ng mlflow.log_dict\n",
    "        mlflow.log_dict(hyperparams, \"model_params.json\")\n",
    "        logger.info(f\"Logged hyperparameters: {hyperparams}\")\n",
    "        \n",
    "        # Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # 5. ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh\n",
    "        logger.info(\"Evaluating model performance...\")\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # TÃ­nh toÃ¡n Metrics\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "        # In vÃ  Log Metrics\n",
    "        metrics = {\n",
    "            \"train_mse\": train_mse, \"train_mae\": train_mae, \"train_r2\": train_r2,\n",
    "            \"test_mse\": test_mse, \"test_mae\": test_mae, \"test_r2\": test_r2,\n",
    "            \"train_rmse\": train_mse**0.5, \"test_rmse\": test_mse**0.5,\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Model performance metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            mlflow.log_metric(key, value)\n",
    "            logger.info(f\" Â {key}: {value:.4f}\")\n",
    "        \n",
    "        # Sá»¬A: Loáº¡i bá» pháº§n log metrics cÅ© trÃ¹ng láº·p\n",
    "        \n",
    "        # 6. LÆ°u trá»¯ Model\n",
    "        \n",
    "        # CÃ¡ch 1: LÆ°u báº±ng joblib (cÃ¡ch truyá»n thá»‘ng)\n",
    "        joblib.dump(model, MODEL_PATH)\n",
    "        mlflow.log_artifact(MODEL_PATH, \"artifacts\")\n",
    "        logger.info(f\"Model saved locally and logged as artifact: {MODEL_PATH}\")\n",
    "        \n",
    "        # CÃ¡ch 2: LÆ°u báº±ng mlflow.sklearn (cÃ¡ch khuyáº¿n nghá»‹)\n",
    "        mlflow.sklearn.log_model(model, \"sklearn_model\")\n",
    "        logger.info(\"Model logged using mlflow.sklearn.log_model\")\n",
    "        \n",
    "    logger.info(f\"\\nâœ… HoÃ n thÃ nh Run ID: {run.info.run_id} trong Experiment: {mlflow.get_experiment(run.info.experiment_id).name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35132a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['d:\\\\Boothcamp_Mlops']\n",
      "INFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [7348] using StatReload\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from scripts.session_3.router import predict, utils\n",
    "\n",
    "app = FastAPI()\n",
    "app.include_router(predict.housing_router)\n",
    "app.include_router(utils.utils_router)\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"Hello, FastAPI!\"}\n",
    "\n",
    "\n",
    "class Method(str, Enum):\n",
    "    add = \"add\"\n",
    "    subtract = \"subtract\"\n",
    "    multiply = \"multiply\"\n",
    "    divide = \"divide\"\n",
    "\n",
    "\n",
    "class CalculateRequest(BaseModel):\n",
    "    method: Method\n",
    "    num1: float\n",
    "    num2: float\n",
    "\n",
    "\n",
    "class CalculateResponse(BaseModel):\n",
    "    result: float\n",
    "\n",
    "\n",
    "@app.post(\"/calculate\", response_model=CalculateResponse)\n",
    "def calculate(request: CalculateRequest) -> CalculateResponse:\n",
    "    if request.method == Method.add:\n",
    "        result = request.num1 + request.num2\n",
    "    elif request.method == Method.subtract:\n",
    "        result = request.num1 - request.num2\n",
    "    elif request.method == Method.multiply:\n",
    "        result = request.num1 * request.num2\n",
    "    elif request.method == Method.divide:\n",
    "        result = request.num1 / request.num2\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid method: {request.method}\")\n",
    "    return CalculateResponse(result=result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"scripts.session_3.api:app\", host=\"0.0.0.0\", port=3000, reload=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
